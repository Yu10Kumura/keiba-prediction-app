"""
Result display component for prediction visualization.

This module provides Streamlit components for displaying prediction results,
performance metrics, and comparative analysis.
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, Any, Optional, List
import logging

logger = logging.getLogger(__name__)


class ResultDisplayComponent:
    """
    Handles display of prediction results and performance metrics.
    
    Provides visualization components for prediction results,
    confidence intervals, and comparative analysis.
    """
    
    def render_prediction_results(
        self, 
        predictions: Dict[str, Any],
        input_data: Optional[pd.DataFrame] = None
    ) -> None:
        """
        Render prediction results with metrics and visualization.
        
        Args:
            predictions: Dictionary containing prediction results
            input_data: Original input data for context
        """
        st.subheader("ğŸ¯ äºˆæ¸¬çµæœ")
        
    def render_prediction_results(
        self, 
        predictions: Dict[str, Any],
        input_data: Optional[pd.DataFrame] = None
    ) -> None:
        """
        Render prediction results with metrics and visualization.
        
        Args:
            predictions: Dictionary containing prediction results
            input_data: Original input data for context
        """
        st.subheader("ğŸ¯ äºˆæ¸¬çµæœ")
        
        if not predictions:
            st.error("âŒ äºˆæ¸¬çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # Handle different prediction result formats
        if isinstance(predictions, dict):
            if 'success' in predictions and not predictions['success']:
                st.error(f"âŒ äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ: {predictions.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
                return
            
            # Get predictions array
            prediction_values = None
            if 'predictions' in predictions:
                prediction_values = predictions['predictions']
            elif 'prediction' in predictions:
                prediction_values = predictions['prediction']
                if not isinstance(prediction_values, list):
                    prediction_values = [prediction_values]
            else:
                st.error("âŒ äºˆæ¸¬çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
        else:
            st.error("âŒ äºˆæ¸¬çµæœã®å½¢å¼ãŒä¸æ­£ã§ã™")
            return
        
        # Create results DataFrame
        horse_names = []
        if input_data is not None and len(input_data) > 0:
            try:
                # Gåˆ—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹6ï¼‰ã‹ã‚‰é¦¬åã‚’ç›´æ¥å–å¾—
                if input_data.shape[1] > 6:
                    horse_names = input_data.iloc[:, 6].astype(str).tolist()
                    # é•·ã•ã‚’äºˆæ¸¬çµæœã«åˆã‚ã›ã‚‹
                    horse_names = horse_names[:len(prediction_values)]
            except Exception:
                horse_names = []
        
        # If no horse names found, create default names
        if not horse_names or len(horse_names) != len(prediction_values):
            horse_names = [f"é¦¬{i}" for i in range(1, len(prediction_values) + 1)]
        
        results_df = pd.DataFrame({
            'é¦¬ç•ª': range(1, len(prediction_values) + 1),
            'é¦¬å': horse_names,
            'äºˆæ¸¬èµ°ç ´ã‚¿ã‚¤ãƒ (ç§’)': [f"{pred:.2f}" for pred in prediction_values],
            'é †ä½äºˆæƒ³': range(1, len(prediction_values) + 1)  # Will be sorted by time
        })
        
        # Sort by predicted time to get ranking
        results_df['äºˆæ¸¬èµ°ç ´ã‚¿ã‚¤ãƒ _æ•°å€¤'] = prediction_values
        results_df = results_df.sort_values('äºˆæ¸¬èµ°ç ´ã‚¿ã‚¤ãƒ _æ•°å€¤').reset_index(drop=True)
        results_df['é †ä½äºˆæƒ³'] = range(1, len(results_df) + 1)
        results_df = results_df.drop('äºˆæ¸¬èµ°ç ´ã‚¿ã‚¤ãƒ _æ•°å€¤', axis=1)
        
        # Display summary metrics
        col1, col2, col3 = st.columns(3)
        
        with col1:
            fastest_time = min(prediction_values)
            st.metric("æœ€é€Ÿäºˆæƒ³ã‚¿ã‚¤ãƒ ", f"{fastest_time:.2f}ç§’")
        
        with col2:
            slowest_time = max(prediction_values)
            st.metric("æœ€é…äºˆæƒ³ã‚¿ã‚¤ãƒ ", f"{slowest_time:.2f}ç§’")
        
        with col3:
            time_range = slowest_time - fastest_time
            st.metric("ã‚¿ã‚¤ãƒ å¹…", f"{time_range:.2f}ç§’")
        
        # Display results table
        st.write("**ğŸ‡ å…¨é¦¬äºˆæ¸¬çµæœ**")
        st.dataframe(
            results_df,
            width='stretch',
            hide_index=True,
            column_config={
                "é¦¬ç•ª": st.column_config.NumberColumn("é¦¬ç•ª", width="small"),
                "é¦¬å": st.column_config.TextColumn("é¦¬å", width="medium"),
                "äºˆæ¸¬èµ°ç ´ã‚¿ã‚¤ãƒ (ç§’)": st.column_config.TextColumn("äºˆæ¸¬èµ°ç ´ã‚¿ã‚¤ãƒ (ç§’)", width="medium"),
                "é †ä½äºˆæƒ³": st.column_config.NumberColumn("é †ä½äºˆæƒ³", width="small")
            }
        )
        
        # Add context if input data is available
        if input_data is not None and len(input_data) > 0:
            st.write("**ğŸ“Š å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æ¦‚è¦**")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("å¯¾è±¡ãƒ¬ãƒ¼ã‚¹é ­æ•°", len(input_data))
            with col2:
                if 'è·é›¢' in input_data.columns:
                    distance = input_data['è·é›¢'].iloc[0] if not input_data['è·é›¢'].empty else "ä¸æ˜"
                    st.metric("ãƒ¬ãƒ¼ã‚¹è·é›¢", f"{distance}m")
        
        # Feature importance if available
        if 'feature_importance' in predictions:
            self._render_feature_importance(predictions['feature_importance'])
    
    def _render_prediction_details(
        self, 
        predictions: Dict[str, Any], 
        input_data: Optional[pd.DataFrame]
    ) -> None:
        """Render detailed prediction information."""
        
        with st.expander("ğŸ“Š äºˆæ¸¬è©³ç´°", expanded=False):
            pred_info = predictions.get('prediction_info', {})
            
            if pred_info:
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**äºˆæ¸¬æƒ…å ±:**")
                    st.write(f"â€¢ ä½¿ç”¨ç‰¹å¾´é‡æ•°: {pred_info.get('num_features', 'N/A')}")
                    st.write(f"â€¢ å‡¦ç†æ™‚é–“: {pred_info.get('processing_time', 'N/A'):.3f}ç§’")
                    st.write(f"â€¢ äºˆæ¸¬æ™‚é–“: {pred_info.get('prediction_time', 'N/A'):.3f}ç§’")
                
                with col2:
                    st.write("**ãƒ¢ãƒ‡ãƒ«æƒ…å ±:**")
                    model_info = predictions.get('model_info', {})
                    st.write(f"â€¢ ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥: {model_info.get('type', 'LightGBM')}")
                    st.write(f"â€¢ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {model_info.get('n_training_samples', 'N/A')}")
                    st.write(f"â€¢ ç‰¹å¾´é‡æ•°: {model_info.get('n_features', 'N/A')}")
            
            # Show input race conditions
            if input_data is not None and len(input_data) > 0:
                st.write("**ãƒ¬ãƒ¼ã‚¹æ¡ä»¶:**")
                race_data = input_data.iloc[0]
                
                conditions = []
                if 'å ´æ‰€' in race_data:
                    conditions.append(f"ç«¶é¦¬å ´: {race_data['å ´æ‰€']}")
                if 'è·é›¢' in race_data:
                    conditions.append(f"è·é›¢: {race_data['è·é›¢']}m")
                if 'èŠãƒ»ãƒ€' in race_data:
                    conditions.append(f"ã‚³ãƒ¼ã‚¹: {race_data['èŠãƒ»ãƒ€']}")
                if 'é¦¬å ´çŠ¶æ…‹' in race_data:
                    conditions.append(f"é¦¬å ´: {race_data['é¦¬å ´çŠ¶æ…‹']}")
                
                for condition in conditions:
                    st.write(f"â€¢ {condition}")
    
    def _render_feature_importance(self, feature_importance: Dict[str, float]) -> None:
        """Render feature importance visualization."""
        
        with st.expander("ğŸ¯ ç‰¹å¾´é‡é‡è¦åº¦", expanded=False):
            if not feature_importance:
                st.write("ç‰¹å¾´é‡é‡è¦åº¦æƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return
            
            # Create feature importance DataFrame
            importance_df = pd.DataFrame([
                {'ç‰¹å¾´é‡': feature, 'é‡è¦åº¦': importance}
                for feature, importance in feature_importance.items()
            ]).sort_values('é‡è¦åº¦', ascending=True)
            
            # Plot horizontal bar chart
            fig = px.bar(
                importance_df,
                x='é‡è¦åº¦',
                y='ç‰¹å¾´é‡',
                orientation='h',
                title='ç‰¹å¾´é‡é‡è¦åº¦',
                labels={'é‡è¦åº¦': 'é‡è¦åº¦', 'ç‰¹å¾´é‡': 'ç‰¹å¾´é‡'}
            )
            
            fig.update_layout(
                height=max(400, len(importance_df) * 25),
                margin=dict(l=150, r=50, t=50, b=50)
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Show top features table
            st.write("**ä¸Šä½ç‰¹å¾´é‡:**")
            top_features = importance_df.tail(10).sort_values('é‡è¦åº¦', ascending=False)
            st.dataframe(top_features, use_container_width=True, hide_index=True)
    
    def _render_time_comparison(self, predicted_time: float, actual_time: float) -> None:
        """Render comparison between predicted and actual times."""
        
        st.subheader("âš–ï¸ å®Ÿç¸¾ã‚¿ã‚¤ãƒ ã¨ã®æ¯”è¼ƒ")
        
        difference = predicted_time - actual_time
        accuracy_pct = (1 - abs(difference) / actual_time) * 100
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "å®Ÿéš›ã®èµ°ç ´ã‚¿ã‚¤ãƒ ",
                f"{actual_time:.2f}ç§’"
            )
        
        with col2:
            delta_color = "inverse" if difference > 0 else "normal"
            st.metric(
                "äºˆæ¸¬ã¨ã®å·®",
                f"{difference:+.2f}ç§’",
                delta=f"{difference:+.2f}ç§’"
            )
        
        with col3:
            st.metric(
                "äºˆæ¸¬ç²¾åº¦",
                f"{accuracy_pct:.1f}%"
            )
        
        # Visualization
        times_df = pd.DataFrame({
            'ã‚¿ã‚¤ãƒ—': ['å®Ÿéš›', 'äºˆæ¸¬'],
            'èµ°ç ´ã‚¿ã‚¤ãƒ ': [actual_time, predicted_time]
        })
        
        fig = px.bar(
            times_df,
            x='ã‚¿ã‚¤ãƒ—',
            y='èµ°ç ´ã‚¿ã‚¤ãƒ ',
            title='èµ°ç ´ã‚¿ã‚¤ãƒ æ¯”è¼ƒ',
            color='ã‚¿ã‚¤ãƒ—',
            color_discrete_map={'å®Ÿéš›': '#ff6b6b', 'äºˆæ¸¬': '#4ecdc4'}
        )
        
        fig.update_layout(
            showlegend=False,
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Accuracy assessment
        if abs(difference) < 1.0:
            st.success(f"âœ… é«˜ç²¾åº¦äºˆæ¸¬ï¼ˆèª¤å·®: {abs(difference):.2f}ç§’ï¼‰")
        elif abs(difference) < 3.0:
            st.info(f"â„¹ï¸ è‰¯å¥½ãªäºˆæ¸¬ï¼ˆèª¤å·®: {abs(difference):.2f}ç§’ï¼‰")
        else:
            st.warning(f"âš ï¸ äºˆæ¸¬èª¤å·®ãŒå¤§ãã„ã§ã™ï¼ˆèª¤å·®: {abs(difference):.2f}ç§’ï¼‰")
    
    def render_batch_results(self, results: List[Dict[str, Any]]) -> None:
        """
        Render results for batch predictions.
        
        Args:
            results: List of prediction result dictionaries
        """
        st.subheader("ğŸ“Š ãƒãƒƒãƒäºˆæ¸¬çµæœ")
        
        if not results:
            st.warning("è¡¨ç¤ºã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # Create summary DataFrame
        summary_data = []
        for i, result in enumerate(results):
            if 'predictions' in result and len(result['predictions']) > 0:
                predicted_time = result['predictions'][0]
                confidence = result.get('confidence', 0)
                
                summary_data.append({
                    'ãƒ¬ãƒ¼ã‚¹ç•ªå·': i + 1,
                    'äºˆæ¸¬ã‚¿ã‚¤ãƒ ': f"{predicted_time:.2f}ç§’",
                    'ä¿¡é ¼åº¦': f"{confidence:.1f}%",
                    'äºˆæ¸¬å€¤': predicted_time  # For sorting
                })
        
        if summary_data:
            summary_df = pd.DataFrame(summary_data)
            
            # Display summary table
            display_df = summary_df.drop('äºˆæ¸¬å€¤', axis=1)
            st.dataframe(display_df, use_container_width=True, hide_index=True)
            
            # Statistics
            col1, col2, col3, col4 = st.columns(4)
            
            predicted_times = [d['äºˆæ¸¬å€¤'] for d in summary_data]
            
            with col1:
                st.metric("äºˆæ¸¬æ•°", len(predicted_times))
            
            with col2:
                st.metric("å¹³å‡ã‚¿ã‚¤ãƒ ", f"{sum(predicted_times)/len(predicted_times):.2f}ç§’")
            
            with col3:
                st.metric("æœ€é€Ÿã‚¿ã‚¤ãƒ ", f"{min(predicted_times):.2f}ç§’")
            
            with col4:
                st.metric("æœ€é…ã‚¿ã‚¤ãƒ ", f"{max(predicted_times):.2f}ç§’")
            
            # Distribution visualization
            fig = px.histogram(
                x=predicted_times,
                nbins=20,
                title='äºˆæ¸¬ã‚¿ã‚¤ãƒ åˆ†å¸ƒ',
                labels={'x': 'äºˆæ¸¬ã‚¿ã‚¤ãƒ ï¼ˆç§’ï¼‰', 'y': 'é »åº¦'}
            )
            
            st.plotly_chart(fig, use_container_width=True)
    
    def render_error_message(self, error_message: str, details: Optional[str] = None) -> None:
        """
        Render error message with optional details.
        
        Args:
            error_message: Main error message
            details: Optional detailed error information
        """
        st.error(f"âŒ {error_message}")
        
        if details:
            with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                st.code(details, language="text")
    
    def render_success_message(self, message: str, details: Optional[Dict[str, Any]] = None) -> None:
        """
        Render success message with optional details.
        
        Args:
            message: Success message
            details: Optional additional information
        """
        st.success(f"âœ… {message}")
        
        if details:
            with st.expander("å‡¦ç†è©³ç´°", expanded=False):
                for key, value in details.items():
                    st.write(f"**{key}:** {value}")
    
    def render_performance_metrics(self, metrics: Dict[str, float]) -> None:
        """
        Render model performance metrics.
        
        Args:
            metrics: Dictionary of performance metrics
        """
        st.subheader("ğŸ“ˆ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æŒ‡æ¨™")
        
        if not metrics:
            st.info("æ€§èƒ½æŒ‡æ¨™ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return
        
        # Display metrics in columns
        metric_cols = st.columns(len(metrics))
        
        for i, (metric_name, metric_value) in enumerate(metrics.items()):
            with metric_cols[i]:
                if isinstance(metric_value, float):
                    st.metric(metric_name, f"{metric_value:.4f}")
                else:
                    st.metric(metric_name, str(metric_value))
        
        # Performance assessment
        if 'rmse' in metrics:
            rmse = metrics['rmse']
            if rmse < 2.0:
                st.success("ğŸ¯ å„ªç§€ãªãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã§ã™")
            elif rmse < 5.0:
                st.info("ğŸ“Š è‰¯å¥½ãªãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã§ã™")
            else:
                st.warning("âš ï¸ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ”¹å–„ãŒå¿…è¦ã§ã™")
