"""
Result display component for prediction visualization.

This module provides Streamlit components for displaying prediction results,
performance metrics, and comparative analysis.
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, Any, Optional, List
import logging

logger = logging.getLogger(__name__)


class ResultDisplayComponent:
    """
    Handles display of prediction results and performance metrics.
    
    Provides visualization components for prediction results,
    confidence intervals, and comparative analysis.
    """
    
    def render_prediction_results(
        self, 
        predictions: Dict[str, Any],
        input_data: Optional[pd.DataFrame] = None
    ) -> None:
        """
        Render prediction results with metrics and visualization.
        
        Args:
            predictions: Dictionary containing prediction results
            input_data: Original input data for context
        """
        st.subheader("ğŸ¯ äºˆæ¸¬çµæœ")
        
        # Debug information
        st.write(f"DEBUG: predictionså‹: {type(predictions)}")
        st.write(f"DEBUG: predictionsã‚­ãƒ¼: {predictions.keys() if isinstance(predictions, dict) else 'Not a dict'}")
        
        if not predictions:
            st.error("âŒ äºˆæ¸¬çµæœãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆäºˆæ¸¬çµæœãŒç©ºã§ã™ï¼‰")
            return
        
        # Handle different prediction result formats
        if isinstance(predictions, dict):
            if 'success' in predictions and not predictions['success']:
                st.error(f"âŒ äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ: {predictions.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
                return
            
            # Try different key formats
            predicted_time = None
            confidence_score = 0
            
            if 'prediction' in predictions:
                predicted_time = predictions['prediction']
            elif 'predictions' in predictions and len(predictions['predictions']) > 0:
                predicted_time = predictions['predictions'][0]
            else:
                st.error("âŒ äºˆæ¸¬çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆpredictionã‚­ãƒ¼ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼‰")
                return
        else:
            st.error("âŒ äºˆæ¸¬çµæœã®å½¢å¼ãŒä¸æ­£ã§ã™")
            return
        
        # Main prediction display
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "äºˆæ¸¬èµ°ç ´ã‚¿ã‚¤ãƒ ",
                f"{predicted_time:.2f}ç§’",
                delta=None
            )
        
        with col2:
            confidence_info = predictions.get('confidence', {})
            if isinstance(confidence_info, dict):
                confidence_score = confidence_info.get('confidence_score', 0) * 100
            else:
                confidence_score = confidence_info * 100 if confidence_info else 0
            
            st.metric(
                "äºˆæ¸¬ä¿¡é ¼åº¦",
                f"{confidence_score:.1f}%",
                delta=None
            )
        
        with col3:
            st.metric(
                "ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥",
                "LightGBM",
                delta=None
            )
        
        # Prediction details
        self._render_prediction_details(predictions, input_data)
        
        # Feature importance if available
        if 'feature_importance' in predictions:
            self._render_feature_importance(predictions['feature_importance'])
    
    def _render_prediction_details(
        self, 
        predictions: Dict[str, Any], 
        input_data: Optional[pd.DataFrame]
    ) -> None:
        """Render detailed prediction information."""
        
        with st.expander("ğŸ“Š äºˆæ¸¬è©³ç´°", expanded=False):
            pred_info = predictions.get('prediction_info', {})
            
            if pred_info:
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**äºˆæ¸¬æƒ…å ±:**")
                    st.write(f"â€¢ ä½¿ç”¨ç‰¹å¾´é‡æ•°: {pred_info.get('num_features', 'N/A')}")
                    st.write(f"â€¢ å‡¦ç†æ™‚é–“: {pred_info.get('processing_time', 'N/A'):.3f}ç§’")
                    st.write(f"â€¢ äºˆæ¸¬æ™‚é–“: {pred_info.get('prediction_time', 'N/A'):.3f}ç§’")
                
                with col2:
                    st.write("**ãƒ¢ãƒ‡ãƒ«æƒ…å ±:**")
                    model_info = predictions.get('model_info', {})
                    st.write(f"â€¢ ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥: {model_info.get('type', 'LightGBM')}")
                    st.write(f"â€¢ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {model_info.get('n_training_samples', 'N/A')}")
                    st.write(f"â€¢ ç‰¹å¾´é‡æ•°: {model_info.get('n_features', 'N/A')}")
            
            # Show input race conditions
            if input_data is not None and len(input_data) > 0:
                st.write("**ãƒ¬ãƒ¼ã‚¹æ¡ä»¶:**")
                race_data = input_data.iloc[0]
                
                conditions = []
                if 'å ´æ‰€' in race_data:
                    conditions.append(f"ç«¶é¦¬å ´: {race_data['å ´æ‰€']}")
                if 'è·é›¢' in race_data:
                    conditions.append(f"è·é›¢: {race_data['è·é›¢']}m")
                if 'èŠãƒ»ãƒ€' in race_data:
                    conditions.append(f"ã‚³ãƒ¼ã‚¹: {race_data['èŠãƒ»ãƒ€']}")
                if 'é¦¬å ´çŠ¶æ…‹' in race_data:
                    conditions.append(f"é¦¬å ´: {race_data['é¦¬å ´çŠ¶æ…‹']}")
                
                for condition in conditions:
                    st.write(f"â€¢ {condition}")
    
    def _render_feature_importance(self, feature_importance: Dict[str, float]) -> None:
        """Render feature importance visualization."""
        
        with st.expander("ğŸ¯ ç‰¹å¾´é‡é‡è¦åº¦", expanded=False):
            if not feature_importance:
                st.write("ç‰¹å¾´é‡é‡è¦åº¦æƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return
            
            # Create feature importance DataFrame
            importance_df = pd.DataFrame([
                {'ç‰¹å¾´é‡': feature, 'é‡è¦åº¦': importance}
                for feature, importance in feature_importance.items()
            ]).sort_values('é‡è¦åº¦', ascending=True)
            
            # Plot horizontal bar chart
            fig = px.bar(
                importance_df,
                x='é‡è¦åº¦',
                y='ç‰¹å¾´é‡',
                orientation='h',
                title='ç‰¹å¾´é‡é‡è¦åº¦',
                labels={'é‡è¦åº¦': 'é‡è¦åº¦', 'ç‰¹å¾´é‡': 'ç‰¹å¾´é‡'}
            )
            
            fig.update_layout(
                height=max(400, len(importance_df) * 25),
                margin=dict(l=150, r=50, t=50, b=50)
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Show top features table
            st.write("**ä¸Šä½ç‰¹å¾´é‡:**")
            top_features = importance_df.tail(10).sort_values('é‡è¦åº¦', ascending=False)
            st.dataframe(top_features, use_container_width=True, hide_index=True)
    
    def _render_time_comparison(self, predicted_time: float, actual_time: float) -> None:
        """Render comparison between predicted and actual times."""
        
        st.subheader("âš–ï¸ å®Ÿç¸¾ã‚¿ã‚¤ãƒ ã¨ã®æ¯”è¼ƒ")
        
        difference = predicted_time - actual_time
        accuracy_pct = (1 - abs(difference) / actual_time) * 100
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "å®Ÿéš›ã®èµ°ç ´ã‚¿ã‚¤ãƒ ",
                f"{actual_time:.2f}ç§’"
            )
        
        with col2:
            delta_color = "inverse" if difference > 0 else "normal"
            st.metric(
                "äºˆæ¸¬ã¨ã®å·®",
                f"{difference:+.2f}ç§’",
                delta=f"{difference:+.2f}ç§’"
            )
        
        with col3:
            st.metric(
                "äºˆæ¸¬ç²¾åº¦",
                f"{accuracy_pct:.1f}%"
            )
        
        # Visualization
        times_df = pd.DataFrame({
            'ã‚¿ã‚¤ãƒ—': ['å®Ÿéš›', 'äºˆæ¸¬'],
            'èµ°ç ´ã‚¿ã‚¤ãƒ ': [actual_time, predicted_time]
        })
        
        fig = px.bar(
            times_df,
            x='ã‚¿ã‚¤ãƒ—',
            y='èµ°ç ´ã‚¿ã‚¤ãƒ ',
            title='èµ°ç ´ã‚¿ã‚¤ãƒ æ¯”è¼ƒ',
            color='ã‚¿ã‚¤ãƒ—',
            color_discrete_map={'å®Ÿéš›': '#ff6b6b', 'äºˆæ¸¬': '#4ecdc4'}
        )
        
        fig.update_layout(
            showlegend=False,
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Accuracy assessment
        if abs(difference) < 1.0:
            st.success(f"âœ… é«˜ç²¾åº¦äºˆæ¸¬ï¼ˆèª¤å·®: {abs(difference):.2f}ç§’ï¼‰")
        elif abs(difference) < 3.0:
            st.info(f"â„¹ï¸ è‰¯å¥½ãªäºˆæ¸¬ï¼ˆèª¤å·®: {abs(difference):.2f}ç§’ï¼‰")
        else:
            st.warning(f"âš ï¸ äºˆæ¸¬èª¤å·®ãŒå¤§ãã„ã§ã™ï¼ˆèª¤å·®: {abs(difference):.2f}ç§’ï¼‰")
    
    def render_batch_results(self, results: List[Dict[str, Any]]) -> None:
        """
        Render results for batch predictions.
        
        Args:
            results: List of prediction result dictionaries
        """
        st.subheader("ğŸ“Š ãƒãƒƒãƒäºˆæ¸¬çµæœ")
        
        if not results:
            st.warning("è¡¨ç¤ºã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # Create summary DataFrame
        summary_data = []
        for i, result in enumerate(results):
            if 'predictions' in result and len(result['predictions']) > 0:
                predicted_time = result['predictions'][0]
                confidence = result.get('confidence', 0)
                
                summary_data.append({
                    'ãƒ¬ãƒ¼ã‚¹ç•ªå·': i + 1,
                    'äºˆæ¸¬ã‚¿ã‚¤ãƒ ': f"{predicted_time:.2f}ç§’",
                    'ä¿¡é ¼åº¦': f"{confidence:.1f}%",
                    'äºˆæ¸¬å€¤': predicted_time  # For sorting
                })
        
        if summary_data:
            summary_df = pd.DataFrame(summary_data)
            
            # Display summary table
            display_df = summary_df.drop('äºˆæ¸¬å€¤', axis=1)
            st.dataframe(display_df, use_container_width=True, hide_index=True)
            
            # Statistics
            col1, col2, col3, col4 = st.columns(4)
            
            predicted_times = [d['äºˆæ¸¬å€¤'] for d in summary_data]
            
            with col1:
                st.metric("äºˆæ¸¬æ•°", len(predicted_times))
            
            with col2:
                st.metric("å¹³å‡ã‚¿ã‚¤ãƒ ", f"{sum(predicted_times)/len(predicted_times):.2f}ç§’")
            
            with col3:
                st.metric("æœ€é€Ÿã‚¿ã‚¤ãƒ ", f"{min(predicted_times):.2f}ç§’")
            
            with col4:
                st.metric("æœ€é…ã‚¿ã‚¤ãƒ ", f"{max(predicted_times):.2f}ç§’")
            
            # Distribution visualization
            fig = px.histogram(
                x=predicted_times,
                nbins=20,
                title='äºˆæ¸¬ã‚¿ã‚¤ãƒ åˆ†å¸ƒ',
                labels={'x': 'äºˆæ¸¬ã‚¿ã‚¤ãƒ ï¼ˆç§’ï¼‰', 'y': 'é »åº¦'}
            )
            
            st.plotly_chart(fig, use_container_width=True)
    
    def render_error_message(self, error_message: str, details: Optional[str] = None) -> None:
        """
        Render error message with optional details.
        
        Args:
            error_message: Main error message
            details: Optional detailed error information
        """
        st.error(f"âŒ {error_message}")
        
        if details:
            with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
                st.code(details, language="text")
    
    def render_success_message(self, message: str, details: Optional[Dict[str, Any]] = None) -> None:
        """
        Render success message with optional details.
        
        Args:
            message: Success message
            details: Optional additional information
        """
        st.success(f"âœ… {message}")
        
        if details:
            with st.expander("å‡¦ç†è©³ç´°", expanded=False):
                for key, value in details.items():
                    st.write(f"**{key}:** {value}")
    
    def render_performance_metrics(self, metrics: Dict[str, float]) -> None:
        """
        Render model performance metrics.
        
        Args:
            metrics: Dictionary of performance metrics
        """
        st.subheader("ğŸ“ˆ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æŒ‡æ¨™")
        
        if not metrics:
            st.info("æ€§èƒ½æŒ‡æ¨™ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return
        
        # Display metrics in columns
        metric_cols = st.columns(len(metrics))
        
        for i, (metric_name, metric_value) in enumerate(metrics.items()):
            with metric_cols[i]:
                if isinstance(metric_value, float):
                    st.metric(metric_name, f"{metric_value:.4f}")
                else:
                    st.metric(metric_name, str(metric_value))
        
        # Performance assessment
        if 'rmse' in metrics:
            rmse = metrics['rmse']
            if rmse < 2.0:
                st.success("ğŸ¯ å„ªç§€ãªãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã§ã™")
            elif rmse < 5.0:
                st.info("ğŸ“Š è‰¯å¥½ãªãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã§ã™")
            else:
                st.warning("âš ï¸ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ”¹å–„ãŒå¿…è¦ã§ã™")
