"""
Data input component for file upload and CSV processing.

This module provides Streamlit components for file upload,
data validation, and initial data processing display.
"""

import streamlit as st
import pandas as pd
import io
from typing import Optional, Tuple, Dict, Any
import logging

from ..constants.validation_rules import FILE_CONSTRAINTS, VALID_VENUES
from ..constants.messages import UI_MESSAGES
from ..core.bloodline_manager import BloodlineManager
from ..core.data_processor import DataProcessor

logger = logging.getLogger(__name__)


class DataInputComponent:
    """
    Handles file upload and data input functionality.
    
    Provides file upload interface, data validation display,
    and initial data processing for CSV files.
    """
    
    def __init__(self):
        """Initialize the data input component."""
        from pathlib import Path
        # Get the bloodline master file path
        master_file_path = Path(__file__).parent.parent.parent / "data" / "bloodline_master.csv"
        self.bloodline_manager = BloodlineManager(str(master_file_path))
        self.data_processor = DataProcessor()
    
    def render_file_upload(self) -> Optional[pd.DataFrame]:
        """
        Render file upload interface.
        
        Returns:
            Uploaded DataFrame or None if no valid file uploaded
        """
        st.subheader("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        
        # File upload widget
        uploaded_file = st.file_uploader(
            "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
            type=['csv'],
            help=f"æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {FILE_CONSTRAINTS['max_file_size'] // (1024*1024)}MB"
        )
        
        if uploaded_file is not None:
            try:
                # File size check
                file_size = len(uploaded_file.getvalue())
                max_size = FILE_CONSTRAINTS['max_file_size']
                
                if file_size > max_size:
                    st.error(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™: {file_size // (1024*1024)}MB > {max_size // (1024*1024)}MB")
                    return None
                
                # Read CSV file
                df = self._read_csv_file(uploaded_file)
                
                if df is not None:
                    st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {len(df)}è¡Œ, {len(df.columns)}åˆ—")
                    
                    # Display data preview
                    self._display_data_preview(df)
                    
                    return df
                
            except Exception as e:
                st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error(f"File upload error: {e}")
        
        return None
    
    def _read_csv_file(self, uploaded_file) -> Optional[pd.DataFrame]:
        """
        Read CSV file with appropriate encoding.
        
        Args:
            uploaded_file: Streamlit uploaded file object
            
        Returns:
            DataFrame or None if read failed
        """
        try:
            # Try different encodings
            encodings = ['utf-8', 'utf-8-sig', 'shift_jis', 'cp932']
            
            for encoding in encodings:
                try:
                    # Reset file pointer
                    uploaded_file.seek(0)
                    
                    # Read CSV
                    df = pd.read_csv(uploaded_file, encoding=encoding)
                    
                    # Basic validation
                    if len(df) == 0:
                        st.warning("âš ï¸ ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™")
                        return None
                    
                    if len(df) > FILE_CONSTRAINTS['max_rows']:
                        st.warning(f"âš ï¸ è¡Œæ•°ãŒå¤šã™ãã¾ã™: {len(df)} > {FILE_CONSTRAINTS['max_rows']}")
                        return df.head(FILE_CONSTRAINTS['max_rows'])
                    
                    st.info(f"ğŸ“‹ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding}")
                    return df
                    
                except UnicodeDecodeError:
                    continue
                    
            st.error("âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã™")
            return None
            
        except Exception as e:
            st.error(f"âŒ CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def _display_data_preview(self, df: pd.DataFrame) -> None:
        """
        Display data preview and basic information.
        
        Args:
            df: DataFrame to preview
        """
        st.subheader("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        # Basic statistics
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("è¡Œæ•°", len(df))
        with col2:
            st.metric("åˆ—æ•°", len(df.columns))
        with col3:
            missing_cells = df.isnull().sum().sum()
            st.metric("æ¬ æå€¤", missing_cells)
        
        # Column information
        st.write("**åˆ—ä¸€è¦§:**")
        column_info = []
        for col in df.columns:
            dtype = str(df[col].dtype)
            null_count = df[col].isnull().sum()
            unique_count = df[col].nunique()
            column_info.append({
                'åˆ—å': col,
                'ãƒ‡ãƒ¼ã‚¿å‹': dtype,
                'æ¬ æå€¤': null_count,
                'ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°': unique_count
            })
        
        st.dataframe(pd.DataFrame(column_info), use_container_width=True)
        
        # Data preview
        st.write("**ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®5è¡Œï¼‰:**")
        st.dataframe(df.head(), use_container_width=True)
    
    def render_data_validation(self, df: pd.DataFrame) -> Tuple[bool, Dict[str, Any]]:
        """
        Render data validation interface and results.
        
        Args:
            df: DataFrame to validate
            
        Returns:
            Tuple of (is_valid, validation_info)
        """
        st.subheader("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³")
        
        # Run validation
        is_valid, errors = self.data_processor.validate_input_data(df)
        
        # Display validation results
        if is_valid:
            st.success("âœ… ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æˆåŠŸ")
        else:
            st.error("âŒ ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼")
            
            st.write("**ã‚¨ãƒ©ãƒ¼è©³ç´°:**")
            for error in errors:
                st.write(f"â€¢ {error}")
        
        # Display column mapping
        self._display_column_mapping(df)
        
        validation_info = {
            'is_valid': is_valid,
            'errors': errors,
            'input_records': len(df)
        }
        
        return is_valid, validation_info
    
    def _display_column_mapping(self, df: pd.DataFrame) -> None:
        """
        Display column mapping and requirements.
        
        Args:
            df: Input DataFrame
        """
        st.write("**åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°çŠ¶æ³:**")
        
        required_columns = [
            'å¹´æœˆæ—¥', 'å ´æ‰€', 'èŠãƒ»ãƒ€', 'è·é›¢', 'é¦¬å ´çŠ¶æ…‹', 'é¦¬ç•ª'
        ]
        
        optional_columns = [
            'é¦¬å', 'çˆ¶é¦¬å', 'æ¯ã®çˆ¶é¦¬å'  # èµ°ç ´ã‚¿ã‚¤ãƒ ã‚’é™¤å¤–
        ]
        
        # Check required columns
        st.write("*å¿…é ˆåˆ—:*")
        for col in required_columns:
            if col in df.columns:
                st.write(f"âœ… {col}")
            else:
                st.write(f"âŒ {col} (ä¸è¶³)")
        
        # Check optional columns
        st.write("*ã‚ªãƒ—ã‚·ãƒ§ãƒ³åˆ—:*")
        for col in optional_columns:
            if col in df.columns:
                st.write(f"âœ… {col}")
            else:
                st.write(f"âšª {col} (ãªã—)")
    
    def render_column_mapping(self, uploaded_df: pd.DataFrame) -> Dict[str, str]:
        """
        Render column mapping interface for CSV upload.
        
        Args:
            uploaded_df: Uploaded DataFrame
            
        Returns:
            Dictionary mapping CSV columns to required columns
        """
        st.subheader("ğŸ“‹ ã‚«ãƒ©ãƒ ãƒãƒƒãƒ”ãƒ³ã‚°")
        st.write("CSVãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ—åã‚’ã‚¢ãƒ—ãƒªã§ä½¿ç”¨ã™ã‚‹åˆ—åã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚")
        
        # å¿…é ˆã‚«ãƒ©ãƒ ï¼ˆèµ°ç ´ã‚¿ã‚¤ãƒ ã‚’é™¤å¤–ï¼‰
        required_columns = {
            'é–‹å‚¬æ—¥': 'year_month_day',
            'ç«¶é¦¬å ´': 'venue', 
            'è·é›¢': 'distance',
            'èŠãƒ»ãƒ€': 'track_type',
            'é¦¬å ´çŠ¶æ…‹': 'track_condition',
            'é¦¬ç•ª': 'horse_number',
            'çˆ¶é¦¬å': 'father_name',
            'æ¯çˆ¶é¦¬å': 'mother_father_name'
        }
        
        csv_columns = ['é¸æŠã—ã¦ãã ã•ã„'] + list(uploaded_df.columns)
        column_mapping = {}
        
        st.write("**å¿…é ˆé …ç›®ã®ãƒãƒƒãƒ”ãƒ³ã‚°:**")
        for display_name, internal_name in required_columns.items():
            selected_column = st.selectbox(
                f"{display_name}:",
                options=csv_columns,
                key=f"mapping_{internal_name}"
            )
            if selected_column != 'é¸æŠã—ã¦ãã ã•ã„':
                column_mapping[internal_name] = selected_column
        
        return column_mapping
    
    def render_data_processing(self, df: pd.DataFrame) -> Tuple[Optional[pd.DataFrame], Dict[str, Any]]:
        """
        Render data processing interface and execute processing.
        
        Args:
            df: Input DataFrame
            
        Returns:
            Tuple of (processed_dataframe, processing_info)
        """
        st.subheader("âš™ï¸ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†")
        
        # Initialize session state for processing results
        if 'processed_data' not in st.session_state:
            st.session_state.processed_data = None
        if 'processing_info' not in st.session_state:
            st.session_state.processing_info = {}
        if 'processing_completed' not in st.session_state:
            st.session_state.processing_completed = False
        
        # Show results if already processed
        if st.session_state.processing_completed and st.session_state.processed_data is not None:
            st.success("âœ… ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            # Display processing results
            st.write("**å‡¦ç†çµæœ:**")
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("å…¥åŠ›ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", st.session_state.processing_info['input_records'])
                st.metric("å‡ºåŠ›ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", st.session_state.processing_info['output_records'])
            
            with col2:
                duration = st.session_state.processing_info.get('processing_duration', 0)
                st.metric("å‡¦ç†æ™‚é–“", f"{duration:.3f}ç§’")
                
                if st.session_state.processing_info['errors']:
                    st.metric("ã‚¨ãƒ©ãƒ¼æ•°", len(st.session_state.processing_info['errors']))
            
            # Show processing steps status
            steps = [
                ('ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³', st.session_state.processing_info['validation_passed']),
                ('ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°', st.session_state.processing_info['cleaning_completed']),
                ('ç‰¹å¾´é‡ç”Ÿæˆ', st.session_state.processing_info['feature_engineering_completed'])
            ]
            
            st.write("**å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—:**")
            for step_name, status in steps:
                status_icon = "âœ…" if status else "âŒ"
                st.write(f"{status_icon} {step_name}")
            
            # Show processed data preview
            if len(st.session_state.processed_data) > 0:
                st.write("**å‡¦ç†å¾Œãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:**")
                st.dataframe(st.session_state.processed_data.head(), use_container_width=True)
            
            # Reset button
            if st.button("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚’å†å‡¦ç†", type="secondary"):
                st.session_state.processing_completed = False
                st.session_state.processed_data = None
                st.session_state.processing_info = {}
                st.rerun()
                
            return st.session_state.processed_data, st.session_state.processing_info
        
        # Process button
        if st.button("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’å®Ÿè¡Œ", type="primary"):
            with st.spinner("å‡¦ç†ä¸­..."):
                # Step 1: Bloodline enrichment
                if 'çˆ¶é¦¬å' in df.columns and 'æ¯ã®çˆ¶é¦¬å' in df.columns:
                    st.write("ğŸ§¬ è¡€çµ±æƒ…å ±ä»˜ä¸ä¸­...")
                    enriched_df = self.bloodline_manager.enrich_dataframe(df)
                    bloodline_added = len(enriched_df.columns) - len(df.columns)
                    st.write(f"âœ… è¡€çµ±åˆ—è¿½åŠ : {bloodline_added}åˆ—")
                else:
                    st.write("âš ï¸ è¡€çµ±æƒ…å ±åˆ—ãŒãªã„ãŸã‚ã€è¡€çµ±ä»˜ä¸ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                    enriched_df = df.copy()
                
                # Step 2: Data processing
                st.write("ğŸ“Š ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¸­...")
                processed_df, processing_info = self.data_processor.process_data(enriched_df)
                
                # Store results in session state
                st.session_state.processed_data = processed_df
                st.session_state.processing_info = processing_info
                st.session_state.processing_completed = True
                
                st.success("âœ… ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
                st.rerun()
        
        return None, {}
    
    def render_sample_data_option(self) -> Optional[pd.DataFrame]:
        """
        Render sample data option for testing.
        
        Returns:
            Sample DataFrame if selected, None otherwise
        """
        st.subheader("ğŸ§ª ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ")
        
        if st.button("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨"):
            from ..core.data_processor import create_sample_data
            sample_df = create_sample_data()
            
            st.success("âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            st.dataframe(sample_df, use_container_width=True)
            
            return sample_df
        
        return None
